{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uWeF_G8hqZ5IAwTXsCQhIqMok-mnVOiY","timestamp":1747945158312},{"file_id":"1mimEoW-lcGnm4vUrF3uV8-dlJ-ETDVJi","timestamp":1747941890633}],"authorship_tag":"ABX9TyOPy+bT5FW/n6z9E850tXVI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Chapter 5 - Obtaining Data"],"metadata":{"id":"V5B0Nz9-zg4Q"}},{"cell_type":"markdown","source":["### 5.1 Introduction\n","\n","This chapter is called Obtaining Data because the word obtaining covers both collecting data (e.g. creating new data) and gaining access to and acquiring existing data. Librarians, in their role as researchers, may collect data as part of a study. As we saw in Chapter 3, in their role providing research services, librarians may assist other researchers to find and use existing data as well as to manage, preserve, and store it.\n","\n","Data collection methods are as myriad as there are research questions and beyond since there will often be multiple ways to answer a research question using multiple types of data depending on the researcher's philosophical approach to research. Considering philosophical approaches to research is extremely important for conducting research but are by and large beyond the scope of this book. While students using this book are strongly recommended to participate in a course devoted to research philosophies and research methods as part of their studies, this book does not assume that the reader has done so. That is one of the primary reasons why the exercises in this book use existing data sets such as the weather data used in chapters 2 and 4.\n","\n","We are interested in how data science techniques may be applied in the context of libraries so we will focus on the kinds of research questions libraries might be interested in. In chapter 3 we reviewed some sources of data that might be relevant to libraries. **In this chapter we will use two methods for obtaining data from the web, webscraping and using an applicaton programming interface (API).**\n","\n","In the first section of this chapter, we will follow a scenario created by Lin and Scott (2023)in their book *Hands on Data Science for Librarians* to learn about web scraping. In their scenario, you are asked to imagine that you are a librarian working in the St Louis Public library. You've been asked to prepare a proposal for presentation to the aldermen on the city council. In the proposal, the library is asking for additional funding for a new library program. As in Lin and Scott's scenario, our first step is to collect contact information for the aldermen from the city's web site using web scraping.\n","\n","In the second section of the chapter we'll learn to use existing data from the U.S. Census. But rather than then collecting data about unemployment rates among adults in the library's service population to learn about downloading Census data (as Lin and Scott do), we will collect data about languages spoken in the homes of children in K-12 public schools in order to plan a collection of new library materials in those languages.\n","\n"],"metadata":{"id":"1rfxsDjTO5SZ"}},{"cell_type":"markdown","source":["### 5.2 Web Scraping\n","\n","Web scraping is literally asking your computer to scrape data from an existing web site. The process is used to obtain a large amount of data in an efficient way and then organize the data for some purpose. The purpose may or may not be to directly answer a reseach question. For example, you might want to scrape book reviews from amazon.com to support collection development.\n","\n","In the scenario we're exploring in this chapter, we're preparing a proposal to request additional funding for a new library program for parents of children in public schools whose primary language spoken at home is not English. We want to share our proposal with all members of the city's board of aldermen (similar to city councils in other places) so we will need to collect their email addresses. Given that there are only 15 alderman for the city of St Louis, we could go to their public facing web pages and download them one by one. But it would be more efficient to simply scrape the data from that web page and then organize that data into a useable format. Learning to do this now will allow us to have a technique in our data science tool kits for next time, when there may be hundreds of pieces of data to be collected.\n","\n","Web scraping requires programming skill that is more complex than what is taught in chapter 2 of this book. Chapter 2 was meant to provide you with enough knowledge to read simple Python code, but not enough to write code for web scraping. Web scraping also requires some understanding of HTML (hyper-text markup language), which is also beyond the scope of this book. So, all of the code for web scraping will be provided and explained here. The reader is encouraged to download **this chapter in its .ipynb form** and run all of the code for themselves. You might even feel confident enough at the end to try it on another web page!\n","\n","The URL to the web page where a list of aldermen for St Louis wards has changed since the Lin and Scott book was published. Since a plan for redistricting has been accomplished (https://www.stlouis-mo.gov/government/departments/aldermen/redistricting/redistricting-2021.cfm), reducing the number of wards from 28 to 14. The current list of 14 aldermen representing the new wards here: https://www.stlouis-mo.gov/government/departments/aldermen/representation/index.cfm.\n","\n"],"metadata":{"id":"xXjfwqPDU9-t"}},{"cell_type":"markdown","source":["We begin by importing the Python script libraries we'll use for web scraping and manipulating the data scraped into a useful form. We're going to use a library you may remember from chapter 2, Pandas, and a couple of new libraries: Requests, BeautifulSoup, and Regular Expressions.\n","\n","The methods in the **Requests library** are used in Python to interact with code written in hyper-text markup language, HTML, which is the language in which many web pages are written. It is particularly useful for fetching data from web pages into Python IDEs. RealPython's [Guide to the Requests Library](https://realpython.com/python-requests/) is a good place to start learning more about it.\n","\n","**BeautifulSoup** \"is a Python package for parsing HTML and XML documents, including those with malformed markup\" (\"BeautifulSoup, HTML Parser,\" 2025). The [BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a good place to learn more about how it works.\n","\n","**Regular expressions** or regex, are \"a sequence of symbols and characters expressing a string or pattern to be searched for within a longer piece of text\" (\"Regular expressions,\" 2025). Geeks for Geeks has a very nice tutorial about [how to use regular expressions](https://www.geeksforgeeks.org/write-regular-expressions/).\n"],"metadata":{"id":"zcImA_TRxvlV"}},{"cell_type":"code","source":["# The Requests library contains functions we'll use in the scraping process.\n","import requests\n","\n","# The Beautiful Soup library contains functions we'll use to pull the data we want from the scraped data.\n","!pip install bs4\n","import requests\n","from bs4 import BeautifulSoup # import BeautifulSoup from bs4\n","\n","# We'll need functions and commands from the Pandas library to create a dataframe containing the data we're seeking\n","import pandas as pd\n","\n","# We'll use regular expressions to separate necessary data from unnecessary data\n","# in the dataframe we create.\n","import re\n"],"metadata":{"id":"FXRftEleOZ8t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we will ontain the html code from the web page we want to scrape, in this case, the list of aldermen to 2024-25 from the page https://www.stlouis-mo.gov/government/departments/aldermen/representation/index.cfm . This creates a Python object in our Colab session that contains the html data from the web page. We're naming that object \"html.\"\n","\n","Don't worry if you can't read the output from this action. Remember that html is a language used mainly for communication between computers. Also notice that the output is lengthy. There is an icon at the top left of the output box where you can hide lengthy output if necessary. Hiding lengthy output sometimes makes maneuvering in the notebook easier.\n","\n","Also notice that in the web version of this chapter I have shortened included in images only part of the full output for the sake of readability."],"metadata":{"id":"8LrpR61jO69O"}},{"cell_type":"code","source":["# prompt: obtain html code from the page https://www.google.com/url?q=https%3A%2F%2Fwww.stlouis-mo.gov%2Fgovernment%2Fdepartments%2Faldermen%2Frepresentation%2Findex.cfm\n","\n","url = 'https://www.stlouis-mo.gov/government/departments/aldermen/representation/index.cfm'\n","response = requests.get(url)\n","\n","if response.status_code == 200:\n","    html = response.text\n","    print(html)\n","else:\n","    print('Error occurred:', response.status_code)\n"],"metadata":{"collapsed":true,"id":"7bQ7Qql7m2db"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we've got the html code from the URL, we'll use commands from BeautifulSoup to extract the data we need, that is the aldermen's names and email addresses, from the rest of the html data.\n","\n","The first thing BeautifulSoup does is create an object called 'soup' that contains the html code from the page we just scraped. This process is sometimes referred to as \"parsing the data.\""],"metadata":{"id":"0g7mE9lByG84"}},{"cell_type":"code","source":["# Parse the HTML content using BeautifulSoup\n","soup = BeautifulSoup(response.content, 'html.parser')\n","\n","print(soup.prettify())"],"metadata":{"collapsed":true,"id":"lUXhJ1m3hFFJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Using some simple commands from the BeautifulSoup library we can look at parts of the html code and isolate the data we need."],"metadata":{"id":"TEdaZwTLy7JP"}},{"cell_type":"code","source":["soup.head"],"metadata":{"collapsed":true,"id":"u1ZbJgRZuw1W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The results still aren't very human readable, even if you are familiar with html. But, if we separate out some of the results piece by piece using BeautifulSoup commands, we can begin to recognize some parts of the web page. Below we'll ask for just the title of the web page we scraped.\n","\n","\n","\n","\n"],"metadata":{"id":"eQbU5R9EQJOj"}},{"cell_type":"code","source":["soup.title"],"metadata":{"id":"ENOFPX_ou6cK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that in the output the html code is slightly more readable. In this case, the title that appears on the web page, \"Aldermen Serving During the 2025-2026 Session,\" is preceded by the word \"title\" enclosed by the greater than and the less than symbols and followed by the text \"/title\" enclosed by the same symbols. That combination of symbols and characters communicates to the computer: here is where the title starts, here is the actual title, and here is where the title ends.\n","\n","This is the basis for the html coding language: instructions to the computer are enclosed between the \"<\" and \">\" symbols. These are called \"html tags.\" There's often, but not always, a beginning tag and an ending tag. Knowing this, we can begin to use those tags to parse out the data we want from the html code. We'll start by looking for a couple other small chunks."],"metadata":{"id":"l2d84q7wUsoM"}},{"cell_type":"code","source":["soup.h1"],"metadata":{"id":"KCFLhuCkvYap"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["soup.h2"],"metadata":{"id":"T1l34lJ8vcGD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["soup.h3"],"metadata":{"id":"bSGEkh2Kvej5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["soup.h4"],"metadata":{"id":"9PIBwwwLwQnx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We find it in the h4 tag. Again using BeautifulSoup methods we can isolate and refine exactly the data we want by specifying the html tag with the h4 tag that we want."],"metadata":{"id":"6my-IqhZzf7u"}},{"cell_type":"code","source":["soup.h4.a"],"metadata":{"id":"6CBEoAg0vgTT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["soup.h4.a.text"],"metadata":{"id":"qTePg1F4wT22"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To retrieve all of the text from each h4 tag, we use the BeautifulSoul method .select."],"metadata":{"id":"nwoRYyy40Aju"}},{"cell_type":"code","source":["names_elements = soup.select('h4')\n","names_elements"],"metadata":{"id":"oGIfv4u10GdY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: from names_elements extract values of a tags\n","\n","aldermen_names = []\n","for element in names_elements:\n","    aldermen_names.append(element.a.text)\n","print(type(aldermen_names))\n","print(aldermen_names)\n"],"metadata":{"id":"Isl00hLd0bSZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: from names_elements extract URLs\n","\n","urls = [element.a['href'] for element in names_elements]\n","print(type(urls))\n","print (urls)"],"metadata":{"id":"b7JAT8vs0Ndj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["At this point we can combine our lists of aldermen names with the URLs that point to their contact information into a data frame using functions and commands from the Pandas library."],"metadata":{"id":"eHwpnLB611gT"}},{"cell_type":"code","source":["# prompt: combine aldermen_names and urls into a dataframe\n","\n","aldermen_df = pd.DataFrame({\n","    'Name': aldermen_names,\n","    'URL': urls\n","})\n","\n","print(aldermen_df.to_string())\n"],"metadata":{"id":"LFb-nrPc1poL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: remove the \\n from aldermen_df['Name']\n","\n","aldermen_df['Name'] = aldermen_df['Name'].str.replace('\\n', '')\n","aldermen_df"],"metadata":{"id":"hZz4PP4w1-Ul"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We'll use the URLs in our dataframe to obtain the aldermen's email addresses, but first, let's add their wards to the dataframe."],"metadata":{"id":"MOOIaokx2P-E"}},{"cell_type":"code","source":["ward_elements = soup.select('span', class_='small')\n","ward_elements\n"],"metadata":{"collapsed":true,"id":"Con7MtoN522L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: obtain tag values from wards_elements\n","\n","wards = []\n","for element in ward_elements:\n","    wards.append(element.text)\n","print(wards)\n"],"metadata":{"id":"husI2szU6Wc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: from wards select only values that begin with \\nWard\n","\n","wards = [ward for ward in wards if ward.startswith('\\nWard')]\n","wards\n"],"metadata":{"id":"6hSn__Wr6gfG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: add wards as a variable to aldermen_df\n","\n","aldermen_df['Ward'] = wards\n","aldermen_df\n"],"metadata":{"id":"LcsVQLrp6t_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: remove the \\n from aldermen_df['Name']\n","\n","aldermen_df['Ward'] = aldermen_df['Ward'].str.replace('\\n', '')\n","aldermen_df"],"metadata":{"id":"zBFk47NB69gv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The last step is to obtain the aldermen's email addresses from their profile web pages. From each page we need the value of the a tag that contains href=mailto:"],"metadata":{"id":"GJUnHAfB7Wto"}},{"cell_type":"code","source":["# prompt: precede each value of aldermen['URL'] precede with https://www.stlouis-mo.gov/ then return the value of the a tags containing href='mailto:\n","\n","emails = []\n","for url in aldermen_df['URL']:\n","  url = 'https://www.stlouis-mo.gov/' + url\n","  response = requests.get(url)\n","  soup = BeautifulSoup(response.content, 'html.parser')\n","  email_element = soup.find('a', href=re.compile('mailto:'))\n","  if email_element:\n","    emails.append(email_element['href'].replace('mailto:', ''))\n","  else:\n","    emails.append('')\n","aldermen_df['Email'] = emails\n","print(aldermen_df)\n"],"metadata":{"id":"p0UFlQl_741U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now that we have our data cleaned and organized in a dataframe, it's probably a good idea to export our dataframe so that we can call it up whenever we need it, rather having to re-process it. Below is the code for saving the dataframe called aldermen-df into a .csv file. If you are exporting to the Colab temporary file storage, don't forget to download the file before leaving Colab."],"metadata":{"id":"yjhA37whburu"}},{"cell_type":"code","source":["# prompt: export aldermen_df to .csv\n","\n","aldermen_df.to_csv('your file path/aldermen.csv', index=False)\n"],"metadata":{"id":"ufyBBbK1cN3F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 5.3 U.S. Census data\n","\n"," According to the Census Bureau's web site, \"Census data covers dozen of topics across 130+ surveys and programs. Get in the weeds with more than 2.5 million tables of raw data, maps, profiles, and more\" (Census, 2025). It is so large that, again, teaching its use is beyond the scope of this book. But it is a treasure trove of data describing U.S. citizens that is often broken down geographically, which makes it an extremely useful tool for libraries wishing to learn more about their populations.\n","\n","To illustrate this, we will continue using our learning scenario in this section. Recall that in this scenario, you were asked to imagine that you are a librarian working in the St Louis Public library. You've been asked to prepare a proposal for presentation to the aldermen on the city council. In the proposal, the library is asking for additional funding for a new library program supporting the people in St Louis who speak a language other than English at home in order to plan a collection of new library materials in those languages.`Specifially, we now want to know what those languages are and how many people speak them. We need a table of the top non-English languages spoken at home by those speak english less than very well (that's table C16001 of the ACS).\n","\n","Using the Census website, [data.census.gov](https://data.census.gov/) we can find the name of the Census table that contains the data we're looking for. It's table C16001, Lanuage Spoken at Home for the Population 5 years and Over. Using the tools at data.census.gove, we are able to narrow the data in table C16001 to the city of St Louis (see footnote below).\n","\n","---\n","*Footnote:* The results of that process can be downloaded from the site, https://data.census.gov/table/ACSDT1Y2021.C16001?g=050XX00US29510, but the aim of this chapter and section is to learn to obtain that data using Python. We will use the site only for reference."],"metadata":{"id":"MJF_Jb6t8u6f"}},{"cell_type":"markdown","source":["#### Obtain the data\n","\n","The first step, in the code block below, is to use Python to obtain the data from the Census site  and use it to create a data frame called census_df."],"metadata":{"id":"-xZtISql0B5E"}},{"cell_type":"code","source":["# prompt: get data from https://api.census.gov/data/2023/acs/acs5?get=group(C16001)&ucgid=0500000US29510\n","\n","import pandas as pd\n","# The URL for the specific Census data you want to access.\n","# This URL points to the C16001 group for the specified UCGID.\n","census_url = \"https://api.census.gov/data/2023/acs/acs5?get=group(C16001)&ucgid=0500000US29510\"\n","\n","# Make a GET request to the Census API.\n","response = requests.get(census_url)\n","\n","# Check if the request was successful (status code 200).\n","if response.status_code == 200:\n","    # Parse the JSON response into a Python dictionary or list.\n","    census_data = response.json()\n","\n","    # The first row of the data is typically the header.\n","    header = census_data[0]\n","\n","    # The rest of the data is the actual data rows.\n","    data_rows = census_data[1:]\n","\n","    # Create a Pandas DataFrame from the data.\n","    census_df = pd.DataFrame(data_rows, columns=header)\n","\n","    # Print the DataFrame.\n","    print(census_df)\n","\n","else:\n","    # If the request was not successful, print the error status code.\n","    print(f\"Error fetching data: {response.status_code}\")\n","    print(response.text) # Print the response text for more details"],"metadata":{"id":"eu7KnlgbwBVW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Clean and Transform the Data\n","\n","We practiced cleaning and transforming data in Chapter 2. In this section we will apply the same ideas to making our data frame of data more easy to read.\n","\n","First, we see that our data frame contains one row with 155 columns. We'd like to have the 155 data points in a single row rather than 155 columns so we'll transpose it."],"metadata":{"id":"N9tfVRgilHXC"}},{"cell_type":"code","source":["# prompt: transpose rows and columns in census_df\n","\n","census_df_transposed = census_df.T\n","census_df_transposed"],"metadata":{"id":"aSOV2O90yb0x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we'll add the index as a second column because it contains the abbreviation for the description of each data point."],"metadata":{"id":"oEArFOvIltro"}},{"cell_type":"code","source":["# prompt: make the index a column in census_df_transpose\n","\n","census_df_transposed = census_df_transposed.reset_index()\n","census_df_transposed"],"metadata":{"id":"kFevVw2v1Yoo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instead of \"Index\" and \"0\", let's change our column (variable) names to 'Name' and 'Value'."],"metadata":{"id":"vXqTmFw2l8et"}},{"cell_type":"code","source":["# prompt: change the names of variables in census_df_transposed to name and value\n","\n","census_df_transposed = census_df_transposed.rename(columns={'index': 'Name', 0: 'Value'})\n","census_df_transposed"],"metadata":{"id":"BTe4gdkz1rS5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we want to replace the data labels (e.g. C16001_038MA) with more human readable labels so  we create a df where the name column has the variable code and the label column has the human readable variable name.\n","\n","Since I have the data labels in a .csv file, I can create a data frame containing them."],"metadata":{"id":"fXzLqIFa0WdP"}},{"cell_type":"code","source":["# Credit for this block of code goes to Tom Mislo, a SLIM student in an earlier iteration of this course.\n","# prompt: data frame called C16001_metadata from /content/ACSDT5Y2023.C16001-Column-Metadata.csv\n","\n","import pandas as pd\n","C16001_metadata = pd.read_csv('/content/ACSDT5Y2023.C16001-Column-Metadata.csv')\n","C16001_metadata"],"metadata":{"id":"XaChxgIiVr9G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that in the census_df_transposed data frame, some of the values in the Name column include an A at the end. Because of that, they won't match identically with the values in the Name column in the C16001_metadata data frame. So we will remove those ending As."],"metadata":{"id":"9jZtNQUYXLd9"}},{"cell_type":"code","source":["# prompt: remove the last character of values in census_df_transposed when that last character is an A\n","\n","census_df_transposed['Name'] = census_df_transposed['Name'].str.rstrip('A')\n","census_df_transposed"],"metadata":{"id":"jIgZt-CBXgmY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Also notice that the first column in C16001_metadata is \"Column Name\". This two words with a space will cause problems as we go on, so we will change it to ColumnName."],"metadata":{"id":"8gq2NXNkX1tr"}},{"cell_type":"code","source":["# prompt: change C16001_metadata[\"Column Name\"] to C16001_metadata[\"Name\"]\n","\n","C16001_metadata = C16001_metadata.rename(columns={'Column Name': 'ColumnName'})\n","print(C16001_metadata.head())\n"],"metadata":{"id":"LhkEX-oBWZ8y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we are ready to add a column to census_df_transposed for the descriptions of the data points. This block of code takes each value in the Name column of the census_df_transposed data frame and looks for it in the Name column of the C16001_metadata data frame. When a match is found, the corresponding value in the Label column of the C16001_metadata data frame is added to a new column in the census_df_transposed dataframe."],"metadata":{"id":"MhhXegnKYTtu"}},{"cell_type":"code","source":["# prompt: when the value in census_df_transposed$Name equals the value in C16006_metadata$ColumnName add a column to census_df_transposed, census_df_transposed$Label that contains the corresponding value from C16006_metadata$Label\n","\n","import pandas as pd\n","census_df_transposed = pd.merge(census_df_transposed, C16001_metadata[['ColumnName', 'Label']], how='left', left_on='Name', right_on='ColumnName')\n","census_df_transposed = census_df_transposed.drop(columns=['ColumnName'])\n","census_df_transposed"],"metadata":{"id":"_Iepdd7WV68-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's shorten the values in the Label column of census_df_transpose data frame to make it a little bit more readable. We'll remove the leading characters that are all the same, Estimate!!Total:!! and replace them with E- and we'll remove the Margin of Error!!Total:!! and replace it with M-."],"metadata":{"id":"octV4oc9Zrz2"}},{"cell_type":"code","source":["# prompt: from census_df_transpose$Label remove Estimate!!Total:!!\n","\n","census_df_transposed['Label'] = census_df_transposed['Label'].str.replace('Estimate!!Total:!!', 'E-', regex=False)\n","census_df_transposed['Label'] = census_df_transposed['Label'].str.replace('Margin of Error!!Total:!!', 'M-', regex=False)\n","census_df_transposed"],"metadata":{"id":"DOw7Wa1CZrBN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For our purpose, to list in numerical order from highest to lowest the languages spoken at home by families in St Louis City who don't speak English well, the only rows from census_df_transpose that we need are those where the Label column\n","\n","* contains the phrase '!!Speak English less than very well', and\n","* contains the characters 'E-'"],"metadata":{"id":"KB76hLvQbQsB"}},{"cell_type":"code","source":["# prompt: new data frame called stl_df that contains rows from census_df_transpose that contain E-\n","\n","stl_df = census_df_transposed[census_df_transposed['Label'].str.contains('!!Speak English less than very well') & census_df_transposed['Label'].str.contains('E-')]\n","stl_df"],"metadata":{"id":"DVJ8XSjpbQhj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can remove those rows that contain None in the Value column and then sort the rest from highest to lowest on the Value column."],"metadata":{"id":"iIByr8I_ccYX"}},{"cell_type":"code","source":["# prompt: remove those rows that contain None in the Value column and then sort the rest from highest to lowest on the Value column.\n","\n","import pandas as pd\n","stl_df = stl_df.dropna(subset=['Value'])\n","stl_df['Value'] = pd.to_numeric(stl_df['Value'])\n","stl_df = stl_df.sort_values(by='Value', ascending=False)\n","stl_df"],"metadata":{"id":"LLkkvJJTbQgp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can see that the language most often spoken at home by those who speak English less than very well is Spanish. In fact, according to this data, there are 2,895 people in this category in the St Louis Public Library's service population. The next largest group is those who speak Veitnamese at home. There are 1,080 of them. This information will be very useful to support our proposal for funding a new library collection to support these potential patrons. There is a bit more cleaning up to do with this table and we'd like to create a visualization of it as well. That will be covered in Chapter 6 on visualizing data."],"metadata":{"id":"jsDF3-ECc5jA"}}]}